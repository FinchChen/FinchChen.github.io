<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="机器学习2. 决策树"><meta name="keywords" content="ML,机器学习"><meta name="author" content="Yifan Chen"><meta name="copyright" content="Yifan Chen"><title>机器学习2. 决策树 | 你好</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#机器学习实战笔记2-决策树"><span class="toc-text">机器学习实战笔记2 决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-概述"><span class="toc-text">1. 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-构造决策树"><span class="toc-text">2. 构造决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-信息增益-information-gain"><span class="toc-text">2.1 信息增益(information gain)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-划分数据集"><span class="toc-text">2.2 划分数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-递归构建决策树"><span class="toc-text">2.3 递归构建决策树</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-用matplotlib注解绘制树形图"><span class="toc-text">3. 用matplotlib注解绘制树形图</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Matplotlib注解"><span class="toc-text">3.1 Matplotlib注解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-测试分类器与存储"><span class="toc-text">4. 测试分类器与存储</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-测试算法"><span class="toc-text">4.1 测试算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-存储与调用"><span class="toc-text">4.2 存储与调用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-实例-用决策树预测隐形眼镜类型"><span class="toc-text">5. 实例 用决策树预测隐形眼镜类型</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">Yifan Chen</div><div class="author-info__description text-center"></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">4</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">2</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">1</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">你好</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span></div><div id="post-info"><div id="post-title">机器学习2. 决策树</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-11-21</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B5%8B%E8%AF%95/">测试</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.7k</span><span class="post-meta__separator">|</span><span>Reading time: 8 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="机器学习实战笔记2-决策树"><a href="#机器学习实战笔记2-决策树" class="headerlink" title="机器学习实战笔记2 决策树"></a>机器学习实战笔记2 决策树</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>优点: 计算复杂度不高,输出结果易于理解,对中间值的缺失不敏感,可以处理不相关特征数据<br>缺点: 容易过拟合</p>
<h2 id="2-构造决策树"><a href="#2-构造决策树" class="headerlink" title="2. 构造决策树"></a>2. 构造决策树</h2><h3 id="2-1-信息增益-information-gain"><a href="#2-1-信息增益-information-gain" class="headerlink" title="2.1 信息增益(information gain)"></a>2.1 信息增益(information gain)</h3><p>构造决策树时,第一个要解决的问题是,当前数据集上哪个特征在划分数据分类时起决定作用. 所以要评估每个特征.(递归)</p>
<p>一些决策树使用二分法,本书使用ID3算法,不生成二叉树.</p>
<p>在划分数据集前后信息发生的变化称为信息增益.计算每个特征分类后的信息增益,最高的则是最好的划分特征.</p>
<p>计算信息增益的方式为香农熵.<strong>熵的定义为 信息的期望值</strong></p>
<p>(数学公式在hexo里面好麻烦..以后有空再补)</p>
<p>香农熵代码实现:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    numEntries = len(dataset)</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataset:</span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>] <span class="comment">#最后一个是标签?</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts: <span class="comment">#遍历</span></span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>) <span class="comment">#信息的定义</span></span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure>
<p>简单来说,熵越高,混合的数据类型也越多.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">creatDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataset = [[<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">0</span>,<span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset, labels</span><br><span class="line"></span><br><span class="line">dataset, labels = creatDataSet()</span><br><span class="line">calcShannonEnt(dataset)</span><br><span class="line"><span class="comment"># 0.9709505944546686</span></span><br><span class="line">dataset[<span class="number">0</span>][<span class="number">-1</span>] = <span class="string">'maybe'</span></span><br><span class="line">calcShannonEnt(dataset)</span><br><span class="line"><span class="comment"># 1.3709505944546687</span></span><br><span class="line">dataset[<span class="number">0</span>][<span class="number">-1</span>] = <span class="string">'no'</span></span><br><span class="line">dataset[<span class="number">1</span>][<span class="number">-1</span>] = <span class="string">'no'</span></span><br><span class="line">calcShannonEnt(dataset)</span><br><span class="line"><span class="comment"># 0.0</span></span><br></pre></td></tr></table></figure>

<p>得到熵之后,就可以按照获取最大信息增益的方法划分数据集.另一个度量集合无序程度的方法是<em>基尼不纯度</em>.书里说略过…</p>
<h3 id="2-2-划分数据集"><a href="#2-2-划分数据集" class="headerlink" title="2.2 划分数据集"></a>2.2 划分数据集</h3><p>按照给定特征划分数据集:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataset, axis, value)</span>:</span></span><br><span class="line">    retDataset = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataset:<span class="comment">#遍历</span></span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value: </span><br><span class="line">            reducedFeatVec = featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:]) <span class="comment">#去掉featVec[axis]这个值</span></span><br><span class="line">            retDataset.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataset <span class="comment">#返回符合条件的数据集</span></span><br></pre></td></tr></table></figure>
<p>遍历,选择最好的数据集划分方式:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    numFeatures = len(dataset[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataset)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">        uniqueVals = set(featList) <span class="comment">#取得唯一值 set里的不重复</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataset = splitDataSet(dataset,i,value)</span><br><span class="line">            prob = len(subDataset)/float(len(dataset))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataset)</span><br><span class="line">        infoGain = baseEntropy - newEntropy <span class="comment">#infogain越大,熵越小 信息增益=熵的减小</span></span><br><span class="line">        <span class="keyword">if</span> infoGain &gt; bestInfoGain:</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure>

<h3 id="2-3-递归构建决策树"><a href="#2-3-递归构建决策树" class="headerlink" title="2.3 递归构建决策树"></a>2.3 递归构建决策树</h3><p>这一节字好多…自己看书吧</p>
<p>当数据集处理完了所有属性,但是子节点依旧含有不同标签,这时就用多数表决的方法决定该子节点的标签</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():</span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),</span><br><span class="line">                             key = operatoe.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>后续还会介绍C4.5,CART等决策树算法,这里先实现ID3算法来构造树:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataset, labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 类别完全相同则停止划分</span></span><br><span class="line">    <span class="keyword">if</span> len(dataset[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataset)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125; <span class="comment"># 嵌套字典</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset,bestFeat,value),subLabels) <span class="comment"># 递归</span></span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset,labels = creatDataSet()</span><br><span class="line">myTree = createTree(dataset, labels)</span><br><span class="line">myTree</span><br><span class="line"><span class="comment"># &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;</span></span><br></pre></td></tr></table></figure>

<hr>
<p>Top-down:</p>
<ol>
<li>选择最好的特征 (chooseBestFeatureToSplit)</li>
<li>递归子树 (createTree)</li>
<li>如果子树全是同一类别就停</li>
<li>所有属性全处理完,就把大部分当作类别 (majorityCnt)</li>
</ol>
<h2 id="3-用matplotlib注解绘制树形图"><a href="#3-用matplotlib注解绘制树形图" class="headerlink" title="3. 用matplotlib注解绘制树形图"></a>3. 用matplotlib注解绘制树形图</h2><p>决策树的主要优点就是直观,易于理解,如果不能直观的将其显示出来,就无法发挥其优势</p>
<h3 id="3-1-Matplotlib注解"><a href="#3-1-Matplotlib注解" class="headerlink" title="3.1 Matplotlib注解"></a>3.1 Matplotlib注解</h3><p>我觉得这个画出来的好丑…就直接贴代码不贴过程了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span>, fc = <span class="string">"0.8"</span>)</span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc= <span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeText, centerPt, parentPt, nodeType)</span>:</span></span><br><span class="line">    createPlot.ax1.annotate(nodeText, xy=parentPt,</span><br><span class="line">                           xycoords=<span class="string">'axes fraction'</span>,xytext = centerPt, textcoords = <span class="string">'axes fraction'</span>,</span><br><span class="line">                           va=<span class="string">"center"</span>, ha = <span class="string">"center"</span>, bbox=nodeType, arrowprops = arrow_args)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">()</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>,facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>)</span><br><span class="line">    plotNode(<span class="string">'decision node'</span>, (<span class="number">0.5</span>,<span class="number">0.1</span>), (<span class="number">0.1</span>,<span class="number">0.5</span>), decisionNode)</span><br><span class="line">    plotNode(<span class="string">'leaf node'</span>, (<span class="number">0.8</span>,<span class="number">0.1</span>), (<span class="number">0.3</span>,<span class="number">0.8</span>), leafNode)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">createPlot()</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/11/23/zBgeUNSrP14tuwC.png" alt="decisiontree1.png"></p>
<p>再添加三个函数,获取叶节点的数目和树的层数,输出预先存储的树的信息</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNumLeafs</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:<span class="comment">#test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:   numLeafs +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTreeDepth</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:<span class="comment">#test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:   thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieveTree</span><span class="params">(i)</span>:</span></span><br><span class="line">    listOfTrees =[&#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                  &#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: &#123;<span class="string">'head'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;, <span class="number">1</span>: <span class="string">'no'</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                  ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br></pre></td></tr></table></figure>
<p>直接上完整版了:<strong>记得把之前的createPlot注释掉</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>]-cntrPt[<span class="number">0</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>]-cntrPt[<span class="number">1</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotTree</span><span class="params">(myTree, parentPt, nodeTxt)</span>:</span><span class="comment">#if the first key tells you what feat was split on</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)  <span class="comment">#this determines the x width of this tree</span></span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]     <span class="comment">#the text label for this node should be this</span></span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + float(numLeafs))/<span class="number">2.0</span>/plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span>/plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:<span class="comment">#test to see if the nodes are dictonaires, if not they are leaf nodes   </span></span><br><span class="line">            plotTree(secondDict[key],cntrPt,str(key))        <span class="comment">#recursion</span></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment">#it's a leaf node print the leaf node</span></span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span>/plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span>/plotTree.totalD</span><br><span class="line"><span class="comment">#if you do get a dictonary you know it's a tree, and the first element will be another dict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">(inTree)</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>, **axprops)    <span class="comment">#no ticks</span></span><br><span class="line">    <span class="comment">#createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses </span></span><br><span class="line">    plotTree.totalW = float(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = <span class="number">-0.5</span>/plotTree.totalW; plotTree.yOff = <span class="number">1.0</span>;</span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>,<span class="number">1.0</span>), <span class="string">''</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myTree = retrieveTree(<span class="number">1</span>)</span><br><span class="line">createPlot(myTree)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/11/23/T2Ix917nLok6jCv.png" alt="decisiontree2.png"></p>
<h2 id="4-测试分类器与存储"><a href="#4-测试分类器与存储" class="headerlink" title="4. 测试分类器与存储"></a>4. 测试分类器与存储</h2><h3 id="4-1-测试算法"><a href="#4-1-测试算法" class="headerlink" title="4.1 测试算法"></a>4.1 测试算法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">    firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict): </span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>: classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"><span class="comment"># classify(mytree, labels, [1,0])</span></span><br></pre></td></tr></table></figure>
<p>缺点: 不能分类[1,3],因为[1,3]不在树里</p>
<h3 id="4-2-存储与调用"><a href="#4-2-存储与调用" class="headerlink" title="4.2 存储与调用"></a>4.2 存储与调用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename,<span class="string">'w'</span>)</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure>

<h2 id="5-实例-用决策树预测隐形眼镜类型"><a href="#5-实例-用决策树预测隐形眼镜类型" class="headerlink" title="5. 实例 用决策树预测隐形眼镜类型"></a>5. 实例 用决策树预测隐形眼镜类型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">'/home/finch/data/Ch03/'</span></span><br><span class="line">fr = open(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>lenses.txt'</span>)</span><br><span class="line">lenses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLables = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'testRate'</span>]</span><br><span class="line">lensesTree = createTree(lenses, lensesLables)</span><br><span class="line">createPlot(lensesTree)</span><br></pre></td></tr></table></figure>
<p><img src="https://i.loli.net/2019/11/23/4uyblHjOM6odLkh.png" alt="decisiontree3.png"></p>
<p><strong>第9章将进一步讨论过拟合与CART算法的问题</strong></p>
</div></article><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ML/">ML</a><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><nav id="pagination"><div class="next-post pull-right"><a href="/2019/11/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.%20KNN/"><span>机器学习1. KNN</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Yifan Chen</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody" target="_blank" rel="noopener"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a href="https://github.com/wzpan/hexo-generator-search" target="_blank" rel="noopener" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>