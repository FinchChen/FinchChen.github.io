<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器学习2. 决策树</title>
      <link href="/2019/11/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02.%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2019/11/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02.%20%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习实战笔记2-决策树"><a href="#机器学习实战笔记2-决策树" class="headerlink" title="机器学习实战笔记2 决策树"></a>机器学习实战笔记2 决策树</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>优点: 计算复杂度不高,输出结果易于理解,对中间值的缺失不敏感,可以处理不相关特征数据<br>缺点: 容易过拟合</p><h2 id="2-构造决策树"><a href="#2-构造决策树" class="headerlink" title="2. 构造决策树"></a>2. 构造决策树</h2><h3 id="2-1-信息增益-information-gain"><a href="#2-1-信息增益-information-gain" class="headerlink" title="2.1 信息增益(information gain)"></a>2.1 信息增益(information gain)</h3><p>构造决策树时,第一个要解决的问题是,当前数据集上哪个特征在划分数据分类时起决定作用. 所以要评估每个特征.(递归)</p><p>一些决策树使用二分法,本书使用ID3算法,不生成二叉树.</p><p>在划分数据集前后信息发生的变化称为信息增益.计算每个特征分类后的信息增益,最高的则是最好的划分特征.</p><p>计算信息增益的方式为香农熵.<strong>熵的定义为 信息的期望值</strong></p><p>(数学公式在hexo里面好麻烦..以后有空再补)</p><p>香农熵代码实现:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcShannonEnt</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    numEntries = len(dataset)</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataset:</span><br><span class="line">        currentLabel = featVec[<span class="number">-1</span>] <span class="comment">#最后一个是标签?</span></span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts: <span class="comment">#遍历</span></span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>) <span class="comment">#信息的定义</span></span><br><span class="line">    <span class="keyword">return</span> shannonEnt</span><br></pre></td></tr></table></figure><p>简单来说,熵越高,混合的数据类型也越多.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">creatDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    dataset = [[<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">1</span>,<span class="string">'yes'</span>],</span><br><span class="line">              [<span class="number">1</span>,<span class="number">0</span>,<span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>],</span><br><span class="line">              [<span class="number">0</span>,<span class="number">1</span>,<span class="string">'no'</span>]]</span><br><span class="line">    labels = [<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>]</span><br><span class="line">    <span class="keyword">return</span> dataset, labels</span><br><span class="line"></span><br><span class="line">dataset, labels = creatDataSet()</span><br><span class="line">calcShannonEnt(dataset)</span><br><span class="line"><span class="comment"># 0.9709505944546686</span></span><br><span class="line">dataset[<span class="number">0</span>][<span class="number">-1</span>] = <span class="string">'maybe'</span></span><br><span class="line">calcShannonEnt(dataset)</span><br><span class="line"><span class="comment"># 1.3709505944546687</span></span><br><span class="line">dataset[<span class="number">0</span>][<span class="number">-1</span>] = <span class="string">'no'</span></span><br><span class="line">dataset[<span class="number">1</span>][<span class="number">-1</span>] = <span class="string">'no'</span></span><br><span class="line">calcShannonEnt(dataset)</span><br><span class="line"><span class="comment"># 0.0</span></span><br></pre></td></tr></table></figure><p>得到熵之后,就可以按照获取最大信息增益的方法划分数据集.另一个度量集合无序程度的方法是<em>基尼不纯度</em>.书里说略过…</p><h3 id="2-2-划分数据集"><a href="#2-2-划分数据集" class="headerlink" title="2.2 划分数据集"></a>2.2 划分数据集</h3><p>按照给定特征划分数据集:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(dataset, axis, value)</span>:</span></span><br><span class="line">    retDataset = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataset:<span class="comment">#遍历</span></span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value: </span><br><span class="line">            reducedFeatVec = featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:]) <span class="comment">#去掉featVec[axis]这个值</span></span><br><span class="line">            retDataset.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> retDataset <span class="comment">#返回符合条件的数据集</span></span><br></pre></td></tr></table></figure><p>遍历,选择最好的数据集划分方式:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">chooseBestFeatureToSplit</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    numFeatures = len(dataset[<span class="number">0</span>]) - <span class="number">1</span></span><br><span class="line">    baseEntropy = calcShannonEnt(dataset)</span><br><span class="line">    bestInfoGain = <span class="number">0.0</span></span><br><span class="line">    bestFeature = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeatures):</span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">        uniqueVals = set(featList) <span class="comment">#取得唯一值 set里的不重复</span></span><br><span class="line">        newEntropy = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">            subDataset = splitDataSet(dataset,i,value)</span><br><span class="line">            prob = len(subDataset)/float(len(dataset))</span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataset)</span><br><span class="line">        infoGain = baseEntropy - newEntropy <span class="comment">#infogain越大,熵越小 信息增益=熵的减小</span></span><br><span class="line">        <span class="keyword">if</span> infoGain &gt; bestInfoGain:</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    <span class="keyword">return</span> bestFeature</span><br></pre></td></tr></table></figure><h3 id="2-3-递归构建决策树"><a href="#2-3-递归构建决策树" class="headerlink" title="2.3 递归构建决策树"></a>2.3 递归构建决策树</h3><p>这一节字好多…自己看书吧</p><p>当数据集处理完了所有属性,但是子节点依旧含有不同标签,这时就用多数表决的方法决定该子节点的标签</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(classList)</span>:</span></span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:</span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():</span><br><span class="line">            classCount[vote] = <span class="number">0</span></span><br><span class="line">        classCount[vote] += <span class="number">1</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),</span><br><span class="line">                             key = operatoe.itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>后续还会介绍C4.5,CART等决策树算法,这里先实现ID3算法来构造树:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTree</span><span class="params">(dataset, labels)</span>:</span></span><br><span class="line">    classList = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == len(classList):</span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 类别完全相同则停止划分</span></span><br><span class="line">    <span class="keyword">if</span> len(dataset[<span class="number">0</span>]) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataset)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125; <span class="comment"># 嵌套字典</span></span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataset]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:</span><br><span class="line">        subLabels = labels[:]</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset,bestFeat,value),subLabels) <span class="comment"># 递归</span></span><br><span class="line">    <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset,labels = creatDataSet()</span><br><span class="line">myTree = createTree(dataset, labels)</span><br><span class="line">myTree</span><br><span class="line"><span class="comment"># &#123;'no surfacing': &#123;0: 'no', 1: &#123;'flippers': &#123;0: 'no', 1: 'yes'&#125;&#125;&#125;&#125;</span></span><br></pre></td></tr></table></figure><hr><p>Top-down:</p><ol><li>选择最好的特征 (chooseBestFeatureToSplit)</li><li>递归子树 (createTree)</li><li>如果子树全是同一类别就停</li><li>所有属性全处理完,就把大部分当作类别 (majorityCnt)</li></ol><h2 id="3-用matplotlib注解绘制树形图"><a href="#3-用matplotlib注解绘制树形图" class="headerlink" title="3. 用matplotlib注解绘制树形图"></a>3. 用matplotlib注解绘制树形图</h2><p>决策树的主要优点就是直观,易于理解,如果不能直观的将其显示出来,就无法发挥其优势</p><h3 id="3-1-Matplotlib注解"><a href="#3-1-Matplotlib注解" class="headerlink" title="3.1 Matplotlib注解"></a>3.1 Matplotlib注解</h3><p>我觉得这个画出来的好丑…就直接贴代码不贴过程了</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span>, fc = <span class="string">"0.8"</span>)</span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span>, fc= <span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotNode</span><span class="params">(nodeText, centerPt, parentPt, nodeType)</span>:</span></span><br><span class="line">    createPlot.ax1.annotate(nodeText, xy=parentPt,</span><br><span class="line">                           xycoords=<span class="string">'axes fraction'</span>,xytext = centerPt, textcoords = <span class="string">'axes fraction'</span>,</span><br><span class="line">                           va=<span class="string">"center"</span>, ha = <span class="string">"center"</span>, bbox=nodeType, arrowprops = arrow_args)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">()</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>,facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>)</span><br><span class="line">    plotNode(<span class="string">'decision node'</span>, (<span class="number">0.5</span>,<span class="number">0.1</span>), (<span class="number">0.1</span>,<span class="number">0.5</span>), decisionNode)</span><br><span class="line">    plotNode(<span class="string">'leaf node'</span>, (<span class="number">0.8</span>,<span class="number">0.1</span>), (<span class="number">0.3</span>,<span class="number">0.8</span>), leafNode)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">createPlot()</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/23/zBgeUNSrP14tuwC.png" alt="decisiontree1.png"></p><p>再添加三个函数,获取叶节点的数目和树的层数,输出预先存储的树的信息</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getNumLeafs</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    numLeafs = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:<span class="comment">#test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:   numLeafs +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> numLeafs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getTreeDepth</span><span class="params">(myTree)</span>:</span></span><br><span class="line">    maxDepth = <span class="number">0</span></span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:<span class="comment">#test to see if the nodes are dictonaires, if not they are leaf nodes</span></span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])</span><br><span class="line">        <span class="keyword">else</span>:   thisDepth = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> thisDepth &gt; maxDepth: maxDepth = thisDepth</span><br><span class="line">    <span class="keyword">return</span> maxDepth</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">retrieveTree</span><span class="params">(i)</span>:</span></span><br><span class="line">    listOfTrees =[&#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;&#125;&#125;,</span><br><span class="line">                  &#123;<span class="string">'no surfacing'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: &#123;<span class="string">'flippers'</span>: &#123;<span class="number">0</span>: &#123;<span class="string">'head'</span>: &#123;<span class="number">0</span>: <span class="string">'no'</span>, <span class="number">1</span>: <span class="string">'yes'</span>&#125;&#125;, <span class="number">1</span>: <span class="string">'no'</span>&#125;&#125;&#125;&#125;</span><br><span class="line">                  ]</span><br><span class="line">    <span class="keyword">return</span> listOfTrees[i]</span><br></pre></td></tr></table></figure><p>直接上完整版了:<strong>记得把之前的createPlot注释掉</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotMidText</span><span class="params">(cntrPt, parentPt, txtString)</span>:</span></span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>]-cntrPt[<span class="number">0</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">0</span>]</span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>]-cntrPt[<span class="number">1</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">1</span>]</span><br><span class="line">    createPlot.ax1.text(xMid, yMid, txtString, va=<span class="string">"center"</span>, ha=<span class="string">"center"</span>, rotation=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotTree</span><span class="params">(myTree, parentPt, nodeTxt)</span>:</span><span class="comment">#if the first key tells you what feat was split on</span></span><br><span class="line">    numLeafs = getNumLeafs(myTree)  <span class="comment">#this determines the x width of this tree</span></span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = list(myTree.keys())[<span class="number">0</span>]     <span class="comment">#the text label for this node should be this</span></span><br><span class="line">    cntrPt = (plotTree.xOff + (<span class="number">1.0</span> + float(numLeafs))/<span class="number">2.0</span>/plotTree.totalW, plotTree.yOff)</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    plotTree.yOff = plotTree.yOff - <span class="number">1.0</span>/plotTree.totalD</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">        <span class="keyword">if</span> type(secondDict[key]).__name__==<span class="string">'dict'</span>:<span class="comment">#test to see if the nodes are dictonaires, if not they are leaf nodes   </span></span><br><span class="line">            plotTree(secondDict[key],cntrPt,str(key))        <span class="comment">#recursion</span></span><br><span class="line">        <span class="keyword">else</span>:   <span class="comment">#it's a leaf node print the leaf node</span></span><br><span class="line">            plotTree.xOff = plotTree.xOff + <span class="number">1.0</span>/plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + <span class="number">1.0</span>/plotTree.totalD</span><br><span class="line"><span class="comment">#if you do get a dictonary you know it's a tree, and the first element will be another dict</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createPlot</span><span class="params">(inTree)</span>:</span></span><br><span class="line">    fig = plt.figure(<span class="number">1</span>, facecolor=<span class="string">'white'</span>)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>, frameon=<span class="literal">False</span>, **axprops)    <span class="comment">#no ticks</span></span><br><span class="line">    <span class="comment">#createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo puropses </span></span><br><span class="line">    plotTree.totalW = float(getNumLeafs(inTree))</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    plotTree.xOff = <span class="number">-0.5</span>/plotTree.totalW; plotTree.yOff = <span class="number">1.0</span>;</span><br><span class="line">    plotTree(inTree, (<span class="number">0.5</span>,<span class="number">1.0</span>), <span class="string">''</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">myTree = retrieveTree(<span class="number">1</span>)</span><br><span class="line">createPlot(myTree)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/23/T2Ix917nLok6jCv.png" alt="decisiontree2.png"></p><h2 id="4-测试分类器与存储"><a href="#4-测试分类器与存储" class="headerlink" title="4. 测试分类器与存储"></a>4. 测试分类器与存储</h2><h3 id="4-1-测试算法"><a href="#4-1-测试算法" class="headerlink" title="4.1 测试算法"></a>4.1 测试算法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(inputTree,featLabels,testVec)</span>:</span></span><br><span class="line">    firstStr = inputTree.keys()[<span class="number">0</span>]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    key = testVec[featIndex]</span><br><span class="line">    valueOfFeat = secondDict[key]</span><br><span class="line">    <span class="keyword">if</span> isinstance(valueOfFeat, dict): </span><br><span class="line">        classLabel = classify(valueOfFeat, featLabels, testVec)</span><br><span class="line">    <span class="keyword">else</span>: classLabel = valueOfFeat</span><br><span class="line">    <span class="keyword">return</span> classLabel</span><br><span class="line"><span class="comment"># classify(mytree, labels, [1,0])</span></span><br></pre></td></tr></table></figure><p>缺点: 不能分类[1,3],因为[1,3]不在树里</p><h3 id="4-2-存储与调用"><a href="#4-2-存储与调用" class="headerlink" title="4.2 存储与调用"></a>4.2 存储与调用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">storeTree</span><span class="params">(inputTree,filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fw = open(filename,<span class="string">'w'</span>)</span><br><span class="line">    pickle.dump(inputTree,fw)</span><br><span class="line">    fw.close()</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grabTree</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> pickle</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)</span><br></pre></td></tr></table></figure><h2 id="5-实例-用决策树预测隐形眼镜类型"><a href="#5-实例-用决策树预测隐形眼镜类型" class="headerlink" title="5. 实例 用决策树预测隐形眼镜类型"></a>5. 实例 用决策树预测隐形眼镜类型</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">PATH = <span class="string">'/home/finch/data/Ch03/'</span></span><br><span class="line">fr = open(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>lenses.txt'</span>)</span><br><span class="line">lenses = [inst.strip().split(<span class="string">'\t'</span>) <span class="keyword">for</span> inst <span class="keyword">in</span> fr.readlines()]</span><br><span class="line">lensesLables = [<span class="string">'age'</span>, <span class="string">'prescript'</span>, <span class="string">'astigmatic'</span>, <span class="string">'testRate'</span>]</span><br><span class="line">lensesTree = createTree(lenses, lensesLables)</span><br><span class="line">createPlot(lensesTree)</span><br></pre></td></tr></table></figure><p><img src="https://i.loli.net/2019/11/23/4uyblHjOM6odLkh.png" alt="decisiontree3.png"></p><p><strong>第9章将进一步讨论过拟合与CART算法的问题</strong></p>]]></content>
      
      
      <categories>
          
          <category> 测试 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习1. KNN</title>
      <link href="/2019/11/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.%20KNN/"/>
      <url>/2019/11/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A01.%20KNN/</url>
      
        <content type="html"><![CDATA[<h1 id="机器学习实战笔记1-K-近邻算法"><a href="#机器学习实战笔记1-K-近邻算法" class="headerlink" title="机器学习实战笔记1 K-近邻算法"></a>机器学习实战笔记1 K-近邻算法</h1><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>KNN算法采用测量不同特征值之间的距离方法进行分类. 一般来说,只选择样本数据集中前k个最相似的数据,来确定新数据的分类.</p><ul><li>优点: 精度高,对异常值不敏感,无数据输入假定</li><li>缺点: 计算复杂度高,空间复杂度高</li></ul><h2 id="2-KNN分类算法实现"><a href="#2-KNN分类算法实现" class="headerlink" title="2. KNN分类算法实现"></a>2. KNN分类算法实现</h2><h3 id="2-1-生成数据集"><a href="#2-1-生成数据集" class="headerlink" title="2.1 生成数据集"></a>2.1 生成数据集</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">creatDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    group = np.array([[<span class="number">1.0</span>,<span class="number">1.1</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]])</span><br><span class="line">    labels = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]</span><br><span class="line">    <span class="keyword">return</span> group, labels</span><br></pre></td></tr></table></figure><h3 id="2-2-分析图像"><a href="#2-2-分析图像" class="headerlink" title="2.2 分析图像"></a>2.2 分析图像</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">dataset, labels = creatDataSet()</span><br><span class="line">plt.scatter(dataset[:,<span class="number">0</span>],dataset[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><h3 id="2-3-kNN分类伪代码"><a href="#2-3-kNN分类伪代码" class="headerlink" title="2.3 kNN分类伪代码"></a>2.3 kNN分类伪代码</h3><pre><code>对未知类别属性的数据集中的每一个点依次执行:1. 计算已知类别数据集中的点与当前点之间的距离2. 按照距离递增次序排序3. 选取与当前点距离最小的k个点4. 确定前k个点所在类别的出现频率5. 返回前k个点出现频率最高的类别当作当前点的预测分类</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classify0</span><span class="params">(inX, dataset, labels, k)</span>:</span></span><br><span class="line">    datasetSize = dataset.shape[<span class="number">0</span>]</span><br><span class="line">    diffMat = np.tile(inX, (datasetSize,<span class="number">1</span>)) - dataset</span><br><span class="line">    <span class="comment"># np.tile(x,shape) 在shape中构造重复个x</span></span><br><span class="line">    <span class="comment"># np.tile([1,2],(3,1)) = [1,2];[1,2];[1,2]</span></span><br><span class="line">    <span class="comment"># diffMat就是距离</span></span><br><span class="line">    sqDiffMat = diffMat ** <span class="number">2</span></span><br><span class="line">    sqDistance = sqDiffMat.sum(axis = <span class="number">1</span>) <span class="comment"># 行距离相加</span></span><br><span class="line">    distances = sqDistance ** <span class="number">0.5</span></span><br><span class="line">    sortedDistIndices = distances.argsort() <span class="comment"># 返回datasetSize个index,[0,1,3,2],越小越近</span></span><br><span class="line">    classCount=&#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">        voteIlabel = labels[sortedDistIndices[i]]</span><br><span class="line">        classCount[voteIlabel] = classCount.get(voteIlabel,<span class="number">0</span>) +<span class="number">1</span> <span class="comment"># classCount &#123;'A':2,'B':1&#125;</span></span><br><span class="line">    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(<span class="number">1</span>),reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># sortedClassCount [('A', 2), ('B', 1)]</span></span><br><span class="line">    <span class="keyword">return</span> sortedClassCount [<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h2 id="3-在约会网站上使用kNN算法"><a href="#3-在约会网站上使用kNN算法" class="headerlink" title="3. 在约会网站上使用kNN算法"></a>3. 在约会网站上使用kNN算法</h2><h3 id="3-1-准备数据"><a href="#3-1-准备数据" class="headerlink" title="3.1 准备数据"></a>3.1 准备数据</h3><p>训练集可在<a href="https://www.manning.com/downloads/1108" target="_blank" rel="noopener"> 这里 </a>获取</p><p>原版书用的是python2.7和numpy,我更倾向于pandas的dataframe,所以就不重复造轮子了.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">PATH = <span class="string">'/home/finch/data/'</span></span><br><span class="line">test = pd.read_table(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>Ch02/datingTestSet2.txt'</span>,header = <span class="literal">None</span>)</span><br><span class="line">datingDataMat = test[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]].copy()</span><br><span class="line">datingLabels = test[[<span class="number">3</span>]].copy()</span><br></pre></td></tr></table></figure><h3 id="3-2-散点图"><a href="#3-2-散点图" class="headerlink" title="3.2 散点图"></a>3.2 散点图</h3><p>这里分别用datingDataMat里的第2,3列和第1,2列画图</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Video Game Percentage (%)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Weekly Icecream Eaten (L)'</span>)</span><br><span class="line"></span><br><span class="line">plt.scatter(datingDataMat[<span class="number">1</span>],datingDataMat[<span class="number">2</span>],</span><br><span class="line">           s = <span class="number">10</span>*np.array(datingLabels[datingLabels.columns[<span class="number">0</span>]]),</span><br><span class="line">           c = np.array(datingLabels[datingLabels.columns[<span class="number">0</span>]]))</span><br><span class="line"><span class="comment"># s 大小, c 颜色</span></span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Yearly Flying Points (miles)'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Video Game Percentage (%)'</span>)</span><br><span class="line">plt.scatter(datingDataMat[<span class="number">0</span>],datingDataMat[<span class="number">1</span>],</span><br><span class="line">           s = <span class="number">8</span>*np.array(datingLabels[datingLabels.columns[<span class="number">0</span>]]),</span><br><span class="line">           c = np.array(datingLabels[datingLabels.columns[<span class="number">0</span>]]))</span><br></pre></td></tr></table></figure><p>得出来的两幅图就跟书上一样了</p><h3 id="3-3-特征值归一化"><a href="#3-3-特征值归一化" class="headerlink" title="3.3 特征值归一化"></a>3.3 特征值归一化</h3><p>与其使用真实值,应该把<del>大的</del><strong>所有的</strong>数值normalize一下.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">datingDataMat = (datingDataMat-datingDataMat.min(<span class="number">0</span>))/datingDataMat.max(<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h3 id="3-4-测试-训练"><a href="#3-4-测试-训练" class="headerlink" title="3.4 测试(训练)"></a>3.4 测试(训练)</h3><p>由于2.2里我使用的是原书上的numpy结构,datingClassTest也需要从dataframe转化成np.array</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">datingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hoRatio = <span class="number">0.1</span></span><br><span class="line">    test = pd.read_table(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>Ch02/datingTestSet2.txt'</span>,header = <span class="literal">None</span>)</span><br><span class="line">    datingDataMat = test[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>]].copy()</span><br><span class="line">    datingLabels = test[[<span class="number">3</span>]].copy()</span><br><span class="line">    normMat = (datingDataMat-datingDataMat.min(<span class="number">0</span>))/datingDataMat.max(<span class="number">0</span>)</span><br><span class="line">    m = normMat.shape[<span class="number">0</span>]</span><br><span class="line">    numTestVecs = int(m*hoRatio)</span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTestVecs): <span class="comment">#随机选10%</span></span><br><span class="line">        classifierResult = classify0(normMat.iloc[i].to_numpy(),normMat.iloc[numTestVecs:m].to_numpy(),datingLabels.iloc[:,<span class="number">0</span>][numTestVecs:m].values,<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult,int(datingLabels.iloc[i])))</span><br><span class="line">        <span class="keyword">if</span> classifierResult != int(datingLabels.iloc[i]):</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"the total error rate is: %f"</span> % (errorCount/float(numTestVecs)))</span><br></pre></td></tr></table></figure><p>得到的准确率接近于94%</p><h2 id="4-使用kNN构建手写识别系统"><a href="#4-使用kNN构建手写识别系统" class="headerlink" title="4. 使用kNN构建手写识别系统"></a>4. 使用kNN构建手写识别系统</h2><p>偷个懒…我就直接复制源码了</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">img2vector</span><span class="params">(filename)</span>:</span></span><br><span class="line">    returnVect = np.zeros((<span class="number">1</span>,<span class="number">1024</span>))</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">        lineStr = fr.readline()</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">32</span>):</span><br><span class="line">            returnVect[<span class="number">0</span>,<span class="number">32</span>*i+j] = int(lineStr[j])</span><br><span class="line">    <span class="keyword">return</span> returnVect</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handwritingClassTest</span><span class="params">()</span>:</span></span><br><span class="line">    hwLabels = []</span><br><span class="line">    trainingFileList = os.listdir(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>Ch02/trainingDigits'</span>)          <span class="comment">#load the training set</span></span><br><span class="line">    m = len(trainingFileList)</span><br><span class="line">    trainingMat = np.zeros((m,<span class="number">1024</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fileNameStr = trainingFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]     <span class="comment">#take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        hwLabels.append(classNumStr)</span><br><span class="line">        trainingMat[i,:] = img2vector(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>Ch02/trainingDigits/%s'</span> % fileNameStr)</span><br><span class="line">    testFileList = os.listdir(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>Ch02/testDigits'</span>)        <span class="comment">#iterate through the test set</span></span><br><span class="line">    errorCount = <span class="number">0.0</span></span><br><span class="line">    mTest = len(testFileList)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(mTest):</span><br><span class="line">        fileNameStr = testFileList[i]</span><br><span class="line">        fileStr = fileNameStr.split(<span class="string">'.'</span>)[<span class="number">0</span>]     <span class="comment">#take off .txt</span></span><br><span class="line">        classNumStr = int(fileStr.split(<span class="string">'_'</span>)[<span class="number">0</span>])</span><br><span class="line">        vectorUnderTest = img2vector(<span class="string">f'<span class="subst">&#123;PATH&#125;</span>Ch02/testDigits/%s'</span> % fileNameStr)</span><br><span class="line">        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"the classifier came back with: %d, the real answer is: %d"</span> % (classifierResult, classNumStr))</span><br><span class="line">        <span class="keyword">if</span> (classifierResult != classNumStr): errorCount += <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"\nthe total number of errors is: %d"</span> % errorCount)</span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"\nthe total error rate is: %f"</span> % (errorCount/float(mTest)))</span><br></pre></td></tr></table></figure><p>核心思想是:</p><ul><li>32x32的txt格式图像</li><li>每个图像转化为1x1024的向量</li><li>然后统计trainingset有m个图片</li><li>于是数据集 = m x 1024的矩阵</li><li>用的是for循环一个一个做的比较…每个做1024*m次浮点运算…所以效率非常低</li></ul><h2 id="5-小结"><a href="#5-小结" class="headerlink" title="5. 小结"></a>5. 小结</h2><p>kNN是最简单的分类算法.其缺点是无法给出任何数据的基础结构信息,因此也无从知晓平均实例样本和典型实例样本具有什么特征<del>这两个玩意是啥</del>.</p>]]></content>
      
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>learning markdown</title>
      <link href="/2019/11/17/test1/"/>
      <url>/2019/11/17/test1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hello world</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/11/17/hello-world/"/>
      <url>/2019/11/17/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
